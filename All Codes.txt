******************** Practical 1 *******************


import tensorflow as tf
print(tf.__version__)
2.12.0
scalar=tf.constant(7)
print(scalar)
print(scalar.ndim)
tf.Tensor(7, shape=(), dtype=int32)
0
vector=tf.constant([10,10])
print(vector)
print(vector.ndim)
tf.Tensor([10 10], shape=(2,), dtype=int32)
1
matrix=tf.constant([
[10,11],[12,13]
    ])
print(matrix)
print(matrix.ndim)
tf.Tensor(
[[10 11]
 [12 13]], shape=(2, 2), dtype=int32)
2
basic_tensor=tf.constant([[10,11],[12,13]])
print(basic_tensor)
tf.Tensor(
[[10 11]
 [12 13]], shape=(2, 2), dtype=int32)
print(basic_tensor+10)
print(basic_tensor-10)
print(basic_tensor*10)
print(basic_tensor/10)
tf.Tensor(
[[20 21]
 [22 23]], shape=(2, 2), dtype=int32)
tf.Tensor(
[[0 1]
 [2 3]], shape=(2, 2), dtype=int32)
tf.Tensor(
[[100 110]
 [120 130]], shape=(2, 2), dtype=int32)
tf.Tensor(
Untitled0.ipynb - Colaboratory https://colab.research.google.com/drive/1F7_eVgydy...
1 of 3 26/07/23, 12:11
tf.Tensor(
[[1. 1.1]
 [1.2 1.3]], shape=(2, 2), dtype=float64)
tensor_011=tf.constant([[2,2],[4,4]])
tensor_012=tf.constant([[2,3],[4,5]])
print(tf.matmul(tensor_011,tensor_012))
tf.Tensor(
[[12 16]
 [24 32]], shape=(2, 2), dtype=int32)
tensor_013 = tf.constant([
[1,2,3],
[4,5,6],
[7,8,9]
],dtype='float32')
print(tf.reduce_min(tensor_013))
print(tf.reduce_max(tensor_013))
print(tf.reduce_sum(tensor_013))
tf.Tensor(1.0, shape=(), dtype=float32)
tf.Tensor(9.0, shape=(), dtype=float32)
tf.Tensor(45.0, shape=(), dtype=float32)
print(tf.sqrt(tensor_013))
print(tf.square(tensor_013))
print(tf.math.log(tensor_013))
tf.Tensor(
[[1. 1.4142135 1.7320508]
 [2. 2.236068 2.4494898]
 [2.6457512 2.828427 3. ]], shape=(3, 3), dtype=float32)
tf.Tensor(
[[ 1. 4. 9.]
 [16. 25. 36.]
 [49. 64. 81.]], shape=(3, 3), dtype=float32)
tf.Tensor(
[[0. 0.6931472 1.0986123]
 [1.3862944 1.609438 1.7917595]
 [1.9459102 2.0794415 2.1972246]], shape=(3, 3), dtype=float32)











*********************************** Practical No 2 *****************************
FFNN


August 7, 2023
[ ]: # Import the necessary packages
[8]: # Importing necessary Libraries
import tensorflow as tf
from tensorflow import keras
[9]: import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
Matplotlib is building the font cache; this may take a moment.
[10]: import os
import sys
os.path.dirname(sys.executable)
[10]: 'C:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311'
[ ]: # Load the training and testing data MNIST
[11]: # Import dataset & split into train and test data
mnist=tf.keras.datasets.mnist
(x_train,y_train),(x_test,y_test)=mnist.load_data()
Downloading data from https://storage.googleapis.com/tensorflow/tf-kerasdatasets/mnist.npz
11490434/11490434 [==============================] - 4s 0us/step
[12]: # Length of the training dataset
len(x_train)
len(y_train)
[12]: 60000
[13]: # Length of the testing dataset
len(x_test)
len(y_test)
1
[13]: 10000
[14]: # Shape of the training dataset
x_train.shape
[14]: (60000, 28, 28)
[15]: # Shape of the testing dataset
x_test.shape
[15]: (10000, 28, 28)
[16]: # See first Image Matrix
x_train[0]
[16]: array([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,
18, 18, 18, 126, 136, 175, 26, 166, 255, 247, 127, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 30, 36, 94, 154, 170,
253, 253, 253, 253, 253, 225, 172, 253, 242, 195, 64, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 49, 238, 253, 253, 253, 253,
253, 253, 253, 253, 251, 93, 82, 82, 56, 39, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 18, 219, 253, 253, 253, 253,
253, 198, 182, 247, 241, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 80, 156, 107, 253, 253,
205, 11, 0, 43, 154, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 1, 154, 253,
90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
2
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 139, 253,
190, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 190,
253, 70, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35,
241, 225, 160, 108, 1, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
81, 240, 253, 253, 119, 25, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 45, 186, 253, 253, 150, 27, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 16, 93, 252, 253, 187, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 249, 253, 249, 64, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 46, 130, 183, 253, 253, 207, 2, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39,
148, 229, 253, 253, 253, 250, 182, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 114, 221,
253, 253, 253, 253, 201, 78, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 23, 66, 213, 253, 253,
253, 253, 198, 81, 2, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 18, 171, 219, 253, 253, 253, 253,
195, 80, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 55, 172, 226, 253, 253, 253, 253, 244, 133,
11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 136, 253, 253, 253, 212, 135, 132, 16, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
3
0, 0],
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0]], dtype=uint8)
[17]: # See first image
plt.matshow(x_train[0])
[17]: <matplotlib.image.AxesImage at 0x2226c857850>
[18]: # Normalize the iamges by scaling pixel intensities to the range 0,1
x_train=x_train/255
x_test=x_test/255
[19]: # See first Naormalize Image Matrix
x_train[0]
[19]: array([[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
4
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0.01176471, 0.07058824, 0.07058824,
0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,
0.65098039, 1. , 0.96862745, 0.49803922, 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0.11764706, 0.14117647,
0.36862745, 0.60392157, 0.66666667, 0.99215686, 0.99215686,
0.99215686, 0.99215686, 0.99215686, 0.88235294, 0.6745098 ,
0.99215686, 0.94901961, 0.76470588, 0.25098039, 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0.19215686, 0.93333333, 0.99215686,
0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,
0.99215686, 0.99215686, 0.98431373, 0.36470588, 0.32156863,
0.32156863, 0.21960784, 0.15294118, 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0.07058824, 0.85882353, 0.99215686,
0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.77647059,
0.71372549, 0.96862745, 0.94509804, 0. , 0. ,
5
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0.31372549, 0.61176471,
0.41960784, 0.99215686, 0.99215686, 0.80392157, 0.04313725,
0. , 0.16862745, 0.60392157, 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0.05490196,
0.00392157, 0.60392157, 0.99215686, 0.35294118, 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0.54509804, 0.99215686, 0.74509804, 0.00784314,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0.04313725, 0.74509804, 0.99215686, 0.2745098 ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0.1372549 , 0.94509804, 0.88235294,
0.62745098, 0.42352941, 0.00392157, 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0.31764706, 0.94117647,
0.99215686, 0.99215686, 0.46666667, 0.09803922, 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0.17647059,
0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
6
0.0627451 , 0.36470588, 0.98823529, 0.99215686, 0.73333333,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0.97647059, 0.99215686, 0.97647059,
0.25098039, 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0.18039216,
0.50980392, 0.71764706, 0.99215686, 0.99215686, 0.81176471,
0.00784314, 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0.15294118, 0.58039216, 0.89803922,
0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.71372549,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,
0.99215686, 0.99215686, 0.78823529, 0.30588235, 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0.09019608, 0.25882353,
0.83529412, 0.99215686, 0.99215686, 0.99215686, 0.99215686,
0.77647059, 0.31764706, 0.00784314, 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0.07058824, 0.67058824, 0.85882353, 0.99215686,
0.99215686, 0.99215686, 0.99215686, 0.76470588, 0.31372549,
0.03529412, 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0.21568627,
0.6745098 , 0.88627451, 0.99215686, 0.99215686, 0.99215686,
0.99215686, 0.95686275, 0.52156863, 0.04313725, 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0.53333333,
0.99215686, 0.99215686, 0.99215686, 0.83137255, 0.52941176,
7
0.51764706, 0.0627451 , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ],
[0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. ]])
[ ]: # Define the network architecture using Keras
[20]: model=keras.Sequential([
# Input Layer
keras.layers.Flatten(input_shape = (28,28)),
# Hidden Layer
keras.layers.Dense(128,activation ='relu'),
# Output Layer
keras.layers.Dense(20,activation = 'softmax')
])
[21]: model.summary()
Model: "sequential"
_________________________________________________________________
Layer (type) Output Shape Param #
=================================================================
flatten (Flatten) (None, 784) 0
dense (Dense) (None, 128) 100480
dense_1 (Dense) (None, 20) 2580
=================================================================
Total params: 103060 (402.58 KB)
8
Trainable params: 103060 (402.58 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
[22]: # Compile the Model
model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',␣
↪metrics=['accuracy'])
[ ]: #Train the model using SGD
[23]: history=model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10)
Epoch 1/10
1875/1875 [==============================] - 13s 4ms/step - loss: 0.6764 -
accuracy: 0.8321 - val_loss: 0.3604 - val_accuracy: 0.9006
Epoch 2/10
1875/1875 [==============================] - 6s 3ms/step - loss: 0.3447 -
accuracy: 0.9031 - val_loss: 0.3004 - val_accuracy: 0.9137
Epoch 3/10
1875/1875 [==============================] - 6s 3ms/step - loss: 0.2967 -
accuracy: 0.9158 - val_loss: 0.2661 - val_accuracy: 0.9245
Epoch 4/10
1875/1875 [==============================] - 6s 3ms/step - loss: 0.2657 -
accuracy: 0.9244 - val_loss: 0.2439 - val_accuracy: 0.9304
Epoch 5/10
1875/1875 [==============================] - 8s 4ms/step - loss: 0.2424 -
accuracy: 0.9318 - val_loss: 0.2252 - val_accuracy: 0.9350
Epoch 6/10
1875/1875 [==============================] - 8s 4ms/step - loss: 0.2227 -
accuracy: 0.9371 - val_loss: 0.2087 - val_accuracy: 0.9397
Epoch 7/10
1875/1875 [==============================] - 8s 4ms/step - loss: 0.2063 -
accuracy: 0.9416 - val_loss: 0.1963 - val_accuracy: 0.9441
Epoch 8/10
1875/1875 [==============================] - 8s 4ms/step - loss: 0.1921 -
accuracy: 0.9462 - val_loss: 0.1861 - val_accuracy: 0.9455
Epoch 9/10
1875/1875 [==============================] - 7s 4ms/step - loss: 0.1803 -
accuracy: 0.9498 - val_loss: 0.1746 - val_accuracy: 0.9485
Epoch 10/10
1875/1875 [==============================] - 7s 4ms/step - loss: 0.1700 -
accuracy: 0.9526 - val_loss: 0.1689 - val_accuracy: 0.9516
[ ]: #Evaluate the network
[24]: test_loss,test_acc=model.evaluate(x_test,y_test)
print("Loss=%.3f" %test_loss)
print("Accuracy=%.3f" %test_acc)
9
313/313 [==============================] - 1s 2ms/step - loss: 0.1689 -
accuracy: 0.9516
Loss=0.169
Accuracy=0.952
[ ]: # Making Prediction on New Data
[25]: n=random.randint(0,9999)
plt.imshow(x_test[n])
plt.show()
[26]: predicted_value=model.predict(x_test)
print("Handwritten number is = %d" %np.argmax(predicted_value[n]))
313/313 [==============================] - 1s 3ms/step
Handwritten number is = 9
[ ]: # Plot the training loss and accuracy
[27]: history.history.keys()
[27]: dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
10
[28]: plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Training Loss & Accuracy')
plt.ylabel('accuracy/loss')
plt.xlabel('epoch')
plt.legend(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
plt.show()
[ ]:
11









*********************************** Practical No 3 *****************************
build an Image classificatin model


import numpy as np
import pandas as pd
import random
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten,Conv2D,Dense,MaxPooling2D
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.datasets import mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11490434/11490434 [==============================] - 0s 0us/step
print(X_train.shape)
(60000, 28, 28)
(60000, 28, 28)
(60000, 28, 28)
X_train[0].min(), X_train[0].max()
(0, 255)
(0, 255)
X_train = (X_train - 0.0) / (255.0 - 0.0)
X_test = (X_test - 0.0) / (255.0 - 0.0)
X_train[0].min(), X_train[0].max()
(0.0, 1.0)
(0.0, 1.0)
def plot_digit(image, digit, plt, i):
  plt.subplot(4, 5, i + 1)
  plt.imshow(image, cmap=plt.get_cmap('gray'))
  plt.title(f"Digit: {digit}")
  plt.xticks([])
  plt.yticks([])
  plt.figure(figsize=(16, 10))
for i in range(20):
  plot_digit(X_train[i], y_train[i], plt, i)
  plt.show()
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
<Figure size 1600x1000 with 0 Axes>
X_train = X_train.reshape((X_train.shape + (1,)))
X_test = X_test.reshape((X_test.shape + (1,)))
import numpy as np
y_train = np.array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9], dtype=np.uint8)
y_train[0:20]
array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9],
 dtype=uint8)
model = Sequential([
Conv2D(32, (3, 3), activation="relu", input_shape=(28, 28, 1)),
MaxPooling2D((2, 2)),
Flatten(),
Dense(100, activation="relu"),
Dense(10, activation="softmax")
])
from tensorflow.keras.optimizers import SGD
optimizer = SGD(learning_rate=0.01, momentum=0.9)
model.compile(optimizer=optimizer,
loss="sparse_categorical_crossentropy",metrics=["accuracy"])
model.summary()
Model: "sequential"
_________________________________________________________________
 Layer (type) Output Shape Param # 
=================================================================
 conv2d (Conv2D) (None, 26, 26, 32) 320 
 
 max_pooling2d (MaxPooling2D (None, 13, 13, 32) 0 
 ) 
 flatten (Flatten) (None, 5408) 0 
 dense (Dense) (None, 100) 540900 
 dense_1 (Dense) (None, 10) 1010 
=================================================================
Total params: 542,230
Trainable params: 542,230
Non-trainable params: 0
_________________________________________________________________
(x_train,y_train),(x_test,y_test) = mnist.load_data()
print(len(x_train), len(y_train))
60000 60000
model.fit(X_train, y_train, epochs=10, batch_size=32)
Epoch 1/10
1875/1875 [==============================] - 39s 20ms/step - loss: 0.2367 - accuracy: 0.9280
Epoch 2/10
1875/1875 [==============================] - 38s 20ms/step - loss: 0.0735 - accuracy: 0.9778
Epoch 3/10
1875/1875 [==============================] - 38s 20ms/step - loss: 0.0475 - accuracy: 0.9855
Epoch 4/10
1875/1875 [==============================] - 39s 21ms/step - loss: 0.0347 - accuracy: 0.9894
Epoch 5/10
1875/1875 [==============================] - 39s 21ms/step - loss: 0.0266 - accuracy: 0.9915
Epoch 6/10
1875/1875 [==============================] - 40s 21ms/step - loss: 0.0204 - accuracy: 0.9934
Epoch 7/10
1875/1875 [==============================] - 38s 20ms/step - loss: 0.0145 - accuracy: 0.9954
Epoch 8/10
1875/1875 [==============================] - 38s 20ms/step - loss: 0.0114 - accuracy: 0.9967
Epoch 9/10
1875/1875 [==============================] - 38s 20ms/step - loss: 0.0080 - accuracy: 0.9980
Epoch 10/10
1875/1875 [==============================] - 37s 20ms/step - loss: 0.0052 - accuracy: 0.9986
<keras.callbacks.History at 0x79b831dbdd80>
plt.figure(figsize=(16, 10))
for i in range(20):
 image = random.choice(X_test).squeeze()
 digit = np.argmax(model.predict(image.reshape((1, 28, 28, 1)))[0],
 axis=-1)
 plot_digit(image, digit, plt, i)
plt.show()
1/1 [==============================] - 0s 21ms/step
1/1 [==============================] - 0s 22ms/step
1/1 [==============================] - 0s 22ms/step
1/1 [==============================] - 0s 22ms/step
1/1 [==============================] - 0s 22ms/step
1/1 [==============================] - 0s 23ms/step
1/1 [==============================] - 0s 30ms/step
1/1 [==============================] - 0s 24ms/step
1/1 [==============================] - 0s 23ms/step
1/1 [==============================] - 0s 22ms/step
1/1 [==============================] - 0s 24ms/step
1/1 [==============================] - 0s 26ms/step
1/1 [==============================] - 0s 26ms/step
1/1 [==============================] - 0s 23ms/step
1/1 [==============================] - 0s 27ms/step
1/1 [==============================] - 0s 21ms/step
1/1 [==============================] - 0s 24ms/step
1/1 [==============================] - 0s 27ms/step
1/1 [==============================] - 0s 21ms/step
1/1 [==============================] - 0s 24ms/step
<ipython-input-14-d3a4bebdd4f8>:7: RuntimeWarning: More than 20 figures have been opened. Figur
 plt.figure(figsize=(16, 10))


<Figure size 1600x1000 with 0 Axes>
predictions = np.argmax(model.predict(X_test), axis=-1)
accuracy_score(y_test, predictions)
313/313 [==============================] - 5s 16ms/step
0.9869
score = model.evaluate(X_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
Test loss: 0.04557980224490166
Test accuracy: 0.9868999719619751
---------------------------------------------------------------------------
AttributeError Traceback (most recent call last)
<ipython-input-83-a44c34ca7a33> in <cell line: 5>()
 3 fig = plt.figure()
 4 plt.subplot(2,1,1)
----> 5 plt.plot(model_log.history['acc'])
 6 plt.plot(model_log.history['val_acc'])
 7 plt.title('model accuracy')
AttributeError: 'dict' object has no attribute 'history'
SEARCH STACK OVERFLOW
import os
import matplotlib.pyplot as plt
fig = plt.figure()
plt.subplot(2,1,1)
plt.plot(model_log.history['acc'])
plt.plot(model_log.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.subplot(2,1,2)
plt.plot(model_log.history['loss'])
plt.plot(model_log.history['val_loss'])
---------------------------------------------------------------------------
AttributeError Traceback (most recent call last)
<ipython-input-81-fe1e2d4f04b1> in <cell line: 2>()
 1 plt.subplot(2,1,2)
----> 2 plt.plot(model_log.history['loss'])
 3 plt.plot(model_log.history['val_loss'])
 4 plt.title('model loss')
 5 plt.ylabel('loss')
AttributeError: 'dict' object has no attribute 'history'
SEARCH STACK OVERFLOW
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.tight_layout()
model_digit_json = model.to_json()
with open("model_digit.json", "w") as json_file:
  json_file.write(model_digit_json)
# serialize weights to HDF5
model.save_weights("model_digit.h5")
print("Saved model to disk")
Saved model to disk









*********************************** Practical No 4 *****************************
Use autoencoders for anomaly detection


Autoencoder anomaly detection
October 2, 2023
[37]: import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, recall_score, accuracy_score,␣
↪precision_score
RANDOM_SEED = 2021
TEST_PCT = 0.3
LABELS = ["Normal","Fraud"]
[38]: dataset = pd.read_csv("F:\College Data\Deep L\PV 4\DL\creditcard.csv")
[39]: #check for any null values
print("Any nulls in the dataset",dataset.isnull().values.any())
print('-------')
print("No. of unique labels",len(dataset['Class'].unique()))
print("Label values",dataset.Class.unique())
#0 is for normal credit card transcation
#1 is for fraudulent credit card transcation
print('-------')
print("Break down of Normal and Fraud Transcations")
print(pd.value_counts(dataset['Class'],sort=True))
Any nulls in the dataset False
-------
No. of unique labels 2
Label values [0 1]
-------
Break down of Normal and Fraud Transcations
Class
0 284315
1 492
1
Name: count, dtype: int64
[40]: #visualizing the imbalanced dataset
count_classes = pd.value_counts(dataset['Class'],sort=True)
count_classes.plot(kind='bar',rot=0)
plt.xticks(range(len(dataset['Class'].unique())),dataset.Class.unique())
plt.title("Frequency by observation number")
plt.xlabel("Class")
plt.ylabel("Number of Observations")
[40]: Text(0, 0.5, 'Number of Observations')
[41]: #Save the normal and fradulent transcations in seperate dataframe
normal_dataset = dataset[dataset.Class == 0]
fraud_dataset = dataset[dataset.Class == 1]
#Visualize transcation amounts for normal and fraudulent transcations
bins = np.linspace(200,2500,100)
plt.hist(normal_dataset.Amount,bins=bins,alpha=1,density=True,label='Normal')
plt.hist(fraud_dataset.Amount,bins=bins,alpha=0.5,density=True,label='Fraud')
2
plt.legend(loc='upper right')
plt.title("Transcation Amount vs Percentage of Transcations")
plt.xlabel("Transcation Amount (USD)")
plt.ylabel("Percentage of Transcations")
plt.show()
[42]: dataset
[42]: Time V1 V2 V3 V4 V5 \
0 0.0 -1.359807 -0.072781 2.536347 1.378155 -0.338321
1 0.0 1.191857 0.266151 0.166480 0.448154 0.060018
2 1.0 -1.358354 -1.340163 1.773209 0.379780 -0.503198
3 1.0 -0.966272 -0.185226 1.792993 -0.863291 -0.010309
4 2.0 -1.158233 0.877737 1.548718 0.403034 -0.407193
… … … … … … …
284802 172786.0 -11.881118 10.071785 -9.834783 -2.066656 -5.364473
284803 172787.0 -0.732789 -0.055080 2.035030 -0.738589 0.868229
284804 172788.0 1.919565 -0.301254 -3.249640 -0.557828 2.630515
284805 172788.0 -0.240440 0.530483 0.702510 0.689799 -0.377961
3
284806 172792.0 -0.533413 -0.189733 0.703337 -0.506271 -0.012546
V6 V7 V8 V9 … V21 V22 \
0 0.462388 0.239599 0.098698 0.363787 … -0.018307 0.277838
1 -0.082361 -0.078803 0.085102 -0.255425 … -0.225775 -0.638672
2 1.800499 0.791461 0.247676 -1.514654 … 0.247998 0.771679
3 1.247203 0.237609 0.377436 -1.387024 … -0.108300 0.005274
4 0.095921 0.592941 -0.270533 0.817739 … -0.009431 0.798278
… … … … … … … …
284802 -2.606837 -4.918215 7.305334 1.914428 … 0.213454 0.111864
284803 1.058415 0.024330 0.294869 0.584800 … 0.214205 0.924384
284804 3.031260 -0.296827 0.708417 0.432454 … 0.232045 0.578229
284805 0.623708 -0.686180 0.679145 0.392087 … 0.265245 0.800049
284806 -0.649617 1.577006 -0.414650 0.486180 … 0.261057 0.643078
V23 V24 V25 V26 V27 V28 Amount \
0 -0.110474 0.066928 0.128539 -0.189115 0.133558 -0.021053 149.62
1 0.101288 -0.339846 0.167170 0.125895 -0.008983 0.014724 2.69
2 0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752 378.66
3 -0.190321 -1.175575 0.647376 -0.221929 0.062723 0.061458 123.50
4 -0.137458 0.141267 -0.206010 0.502292 0.219422 0.215153 69.99
… … … … … … … …
284802 1.014480 -0.509348 1.436807 0.250034 0.943651 0.823731 0.77
284803 0.012463 -1.016226 -0.606624 -0.395255 0.068472 -0.053527 24.79
284804 -0.037501 0.640134 0.265745 -0.087371 0.004455 -0.026561 67.88
284805 -0.163298 0.123205 -0.569159 0.546668 0.108821 0.104533 10.00
284806 0.376777 0.008797 -0.473649 -0.818267 -0.002415 0.013649 217.00
Class
0 0
1 0
2 0
3 0
4 0
… …
284802 0
284803 0
284804 0
284805 0
284806 0
[284807 rows x 31 columns]
[43]: sc = StandardScaler()
dataset['Time'] = sc.fit_transform(dataset['Time'].values.reshape(-1,1))
dataset['Amount'] = sc.fit_transform(dataset['Amount'].values.reshape(-1,1))
4
[44]: raw_data = dataset.values
#The last element contains if the transcation is normal which is represented by␣
↪0 and if fraud then 1
labels = raw_data[:,-1]
#The other data points are the electrocadriogram data
data = raw_data[:,0:-1]
train_data,test_data,train_labels,test_labels =␣
↪train_test_split(data,labels,test_size = 0.2,random_state =2021)
[45]: min_val = tf.reduce_min(train_data)
max_val = tf.reduce_max(train_data)
train_data = (train_data - min_val) / (max_val - min_val)
test_data = (test_data - min_val) / (max_val - min_val)
train_data = tf.cast(train_data,tf.float32)
test_data = tf.cast(test_data,tf.float32)
[46]: train_labels = train_labels.astype(bool)
test_labels = test_labels.astype(bool)
#Creating normal and fraud datasets
normal_train_data = train_data[~train_labels]
normal_test_data = test_data[~test_labels]
fraud_train_data = train_data[train_labels]
fraud_test_data = test_data[test_labels]
print("No. of records in Fraud Train Data=",len(fraud_train_data))
print("No. of records in Normal Train Data=",len(normal_train_data))
print("No. of records in Fraud Test Data=",len(fraud_test_data))
print("No. of records in Normal Test Data=",len(normal_test_data))
No. of records in Fraud Train Data= 389
No. of records in Normal Train Data= 227456
No. of records in Fraud Test Data= 103
No. of records in Normal Test Data= 56859
[47]: nb_epoch = 20
batch_size = 64
input_dim = normal_train_data.shape[1]
#num of columns,30
encoding_dim = 14
hidden_dim1 = int(encoding_dim / 2)
hidden_dim2 = 4
learning_rate = 1e-7
5
[48]: #input layer
input_layer = tf.keras.layers.Input(shape=(input_dim,))
#Encoder
encoder = tf.keras.layers.
↪Dense(encoding_dim,activation="tanh",activity_regularizer = tf.keras.
↪regularizers.l2(learning_rate))(input_layer)
encoder = tf.keras.layers.Dropout(0.2)(encoder)
encoder = tf.keras.layers.Dense(hidden_dim1,activation='relu')(encoder)
encoder = tf.keras.layers.Dense(hidden_dim2,activation=tf.nn.
↪leaky_relu)(encoder)
#Decoder
decoder = tf.keras.layers.Dense(hidden_dim1,activation='relu')(encoder)
decoder = tf.keras.layers.Dropout(0.2)(decoder)
decoder = tf.keras.layers.Dense(encoding_dim,activation='relu')(decoder)
decoder = tf.keras.layers.Dense(input_dim,activation='tanh')(decoder)
#Autoencoder
autoencoder = tf.keras.Model(inputs = input_layer,outputs = decoder)
autoencoder.summary()
Model: "model_2"
_________________________________________________________________
Layer (type) Output Shape Param #
=================================================================
input_3 (InputLayer) [(None, 30)] 0
dense_12 (Dense) (None, 14) 434
dropout_4 (Dropout) (None, 14) 0
dense_13 (Dense) (None, 7) 105
dense_14 (Dense) (None, 4) 32
dense_15 (Dense) (None, 7) 35
dropout_5 (Dropout) (None, 7) 0
dense_16 (Dense) (None, 14) 112
dense_17 (Dense) (None, 30) 450
=================================================================
Total params: 1168 (4.56 KB)
Trainable params: 1168 (4.56 KB)
Non-trainable params: 0 (0.00 Byte)
6
_________________________________________________________________
[49]: cp = tf.keras.callbacks.ModelCheckpoint(filepath="autoencoder_fraud.
↪h5",mode='min',monitor='val_loss',verbose=2,save_best_only=True)
#Define our early stopping
early_stop = tf.keras.callbacks.EarlyStopping(
monitor='val_loss',
min_delta=0.0001,
patience=10,
verbose=11,
mode='min',
restore_best_weights=True
)
[50]: autoencoder.compile(metrics=['accuracy'],loss=␣
↪'mean_squared_error',optimizer='adam')
[51]: history = autoencoder.fit(normal_train_data,normal_train_data,epochs = nb_epoch,
batch_size = batch_size,shuffle = True,
validation_data = (test_data,test_data),
verbose=1,
callbacks = [cp,early_stop]).history
Epoch 1/20
3545/3554 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy:
0.0305
Epoch 1: val_loss improved from inf to 0.00003, saving model to
autoencoder_fraud.h5
C:\Users\Acer\AppData\Local\Programs\Python\Python311\Lib\sitepackages\keras\src\engine\training.py:3000: UserWarning: You are saving your
model as an HDF5 file via `model.save()`. This file format is considered legacy.
We recommend using instead the native Keras format, e.g.
`model.save('my_model.keras')`.
saving_api.save_model(
3554/3554 [==============================] - 14s 3ms/step - loss: 0.0035 -
accuracy: 0.0304 - val_loss: 2.5545e-05 - val_accuracy: 0.0024
Epoch 2/20
3541/3554 [============================>.] - ETA: 0s - loss: 1.9480e-05 -
accuracy: 0.0564
Epoch 2: val_loss improved from 0.00003 to 0.00002, saving model to
autoencoder_fraud.h5
3554/3554 [==============================] - 11s 3ms/step - loss: 1.9475e-05 -
accuracy: 0.0563 - val_loss: 2.3230e-05 - val_accuracy: 0.0024
Epoch 3/20
3548/3554 [============================>.] - ETA: 0s - loss: 1.9425e-05 -
accuracy: 0.0657
Epoch 3: val_loss improved from 0.00002 to 0.00002, saving model to
7
autoencoder_fraud.h5
3554/3554 [==============================] - 12s 3ms/step - loss: 1.9421e-05 -
accuracy: 0.0658 - val_loss: 2.2306e-05 - val_accuracy: 0.0010
Epoch 4/20
3547/3554 [============================>.] - ETA: 0s - loss: 1.9538e-05 -
accuracy: 0.0587
Epoch 4: val_loss improved from 0.00002 to 0.00002, saving model to
autoencoder_fraud.h5
3554/3554 [==============================] - 12s 3ms/step - loss: 1.9538e-05 -
accuracy: 0.0586 - val_loss: 2.0339e-05 - val_accuracy: 0.0024
Epoch 5/20
3553/3554 [============================>.] - ETA: 0s - loss: 1.9482e-05 -
accuracy: 0.0615
Epoch 5: val_loss did not improve from 0.00002
3554/3554 [==============================] - 11s 3ms/step - loss: 1.9483e-05 -
accuracy: 0.0615 - val_loss: 2.0421e-05 - val_accuracy: 0.0661
Epoch 6/20
3552/3554 [============================>.] - ETA: 0s - loss: 1.9032e-05 -
accuracy: 0.0696
Epoch 6: val_loss did not improve from 0.00002
3554/3554 [==============================] - 11s 3ms/step - loss: 1.9030e-05 -
accuracy: 0.0695 - val_loss: 2.1059e-05 - val_accuracy: 0.0514
Epoch 7/20
3551/3554 [============================>.] - ETA: 0s - loss: 1.8035e-05 -
accuracy: 0.1084
Epoch 7: val_loss did not improve from 0.00002
3554/3554 [==============================] - 11s 3ms/step - loss: 1.8035e-05 -
accuracy: 0.1084 - val_loss: 2.3646e-05 - val_accuracy: 0.0257
Epoch 8/20
3538/3554 [============================>.] - ETA: 0s - loss: 1.7710e-05 -
accuracy: 0.1286
Epoch 8: val_loss did not improve from 0.00002
3554/3554 [==============================] - 12s 3ms/step - loss: 1.7702e-05 -
accuracy: 0.1286 - val_loss: 2.5736e-05 - val_accuracy: 0.0257
Epoch 9/20
3541/3554 [============================>.] - ETA: 0s - loss: 1.7467e-05 -
accuracy: 0.1511
Epoch 9: val_loss did not improve from 0.00002
3554/3554 [==============================] - 12s 3ms/step - loss: 1.7465e-05 -
accuracy: 0.1511 - val_loss: 2.5645e-05 - val_accuracy: 0.0270
Epoch 10/20
3552/3554 [============================>.] - ETA: 0s - loss: 1.7224e-05 -
accuracy: 0.1891
Epoch 10: val_loss did not improve from 0.00002
3554/3554 [==============================] - 12s 3ms/step - loss: 1.7226e-05 -
accuracy: 0.1891 - val_loss: 2.7170e-05 - val_accuracy: 0.0284
Epoch 11/20
3548/3554 [============================>.] - ETA: 0s - loss: 1.6969e-05 -
8
accuracy: 0.2355
Epoch 11: val_loss did not improve from 0.00002
Restoring model weights from the end of the best epoch: 1.
3554/3554 [==============================] - 12s 3ms/step - loss: 1.6970e-05 -
accuracy: 0.2356 - val_loss: 2.4134e-05 - val_accuracy: 0.0262
Epoch 11: early stopping
[52]: plt.plot(history['loss'],linewidth = 2,label = 'Train')
plt.plot(history['val_loss'],linewidth = 2,label = 'Test')
plt.legend(loc='upper right')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
#plt.ylim(ymin=0.70,ymax=1)
plt.show()
[53]: test_x_predictions = autoencoder.predict(test_data)
mse = np.mean(np.power(test_data - test_x_predictions, 2),axis = 1)
9
error_df = pd.DataFrame({'Reconstruction_error':mse,
'True_class':test_labels})
1781/1781 [==============================] - 3s 2ms/step
[54]: threshold_fixed = 50
groups = error_df.groupby('True_class')
fig,ax = plt.subplots()
for name,group in groups:
ax.plot(group.index,group.Reconstruction_error,marker='o',ms = 3.
↪5,linestyle='',
label = "Fraud" if name==1 else "Normal")
ax.hlines(threshold_fixed,ax.get_xlim()[0],ax.
↪get_xlim()[1],colors="r",zorder=100,label="Threshold")
ax.legend()
plt.title("Reconstructions error for normal and fraud data")
plt.ylabel("Reconstruction error")
plt.xlabel("Data point index")
plt.show()
10
[55]: threshold_fixed = 52
pred_y = [1 if e > threshold_fixed else 0
for e in
error_df.Reconstruction_error.values]
error_df['pred'] = pred_y
conf_matrix = confusion_matrix(error_df.True_class,pred_y)
plt.figure(figsize = (4,4))
sns.heatmap(conf_matrix,xticklabels = LABELS,yticklabels = LABELS,annot =␣
↪True,fmt="d")
plt.title("Confusion matrix")
plt.ylabel("True class")
plt.xlabel("Predicted class")
plt.show()
#Print Accuracy,Precision and Recall
print("Accuracy :",accuracy_score(error_df['True_class'],error_df['pred']))
print("Recall :",recall_score(error_df['True_class'],error_df['pred']))
print("Precision :",precision_score(error_df['True_class'],error_df['pred']))
Accuracy : 0.9981917769741231
Recall : 0.0
Precision : 0.0
11
C:\Users\Acer\AppData\Local\Programs\Python\Python311\Lib\sitepackages\sklearn\metrics\_classification.py:1469: UndefinedMetricWarning:
Precision is ill-defined and being set to 0.0 due to no predicted samples. Use
`zero_division` parameter to control this behavior.
_warn_prf(average, modifier, msg_start, len(result))
[ ]:
[ ]:
12




OR




/***Sample code****/
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
pip install tensorflow --user
!pip install keras
!pip install daytime
!pip install torch
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, recall_score, accuracy_score,
precision_score
RANDOM_SEED = 2021
TEST_PCT = 0.3
LABELS = ["Normal","Fraud"]
dataset = pd.read_csv("E:\Teachning material\Deep learning BE IT 2019 course
\creditcard.csv")
#dataset.head
print(list(dataset.columns))
dataset.describe()
#check for any nullvalues
print("Any nulls in the dataset ",dataset.isnull().values.any() )
print('-------')
print("No. of unique labels ", len(dataset['Class'].unique()))
print("Label values ",dataset.Class.unique())
#0 is for normal credit card transaction
#1 is for fraudulent credit card transaction
print('-------')
print("Break down of the Normal and Fraud Transactions")
print(pd.value_counts(dataset['Class'], sort = True) )
#Visualizing the imbalanced dataset
count_classes = pd.value_counts(dataset['Class'], sort = True)
count_classes.plot(kind = 'bar', rot=0)
plt.xticks(range(len(dataset['Class'].unique())), dataset.Class.unique())
plt.title("Frequency by observation number")
plt.xlabel("Class")
plt.ylabel("Number of Observations");
# Save the normal and fradulent transactions in separate dataframe
Page 4 of 13
normal_dataset = dataset[dataset.Class == 0]
fraud_dataset = dataset[dataset.Class == 1]
#Visualize transactionamounts for normal and fraudulent transactions
bins = np.linspace(200, 2500, 100)
plt.hist(normal_dataset.Amount, bins=bins, alpha=1, density=True, label='Nor
mal')
plt.hist(fraud_dataset.Amount, bins=bins, alpha=0.5, density=True, label='Fr
aud')
plt.legend(loc='upper right')
plt.title("Transaction amount vs Percentage of transactions")
plt.xlabel("Transaction amount (USD)")
plt.ylabel("Percentage of transactions");
plt.show()
#'''Time and Amount are the columns that are not scaled, so applying Standar
dScaler to only Amount and Time columns.
Normalizing the values between 0 and 1 did not work great for the dataset.''
'
sc=StandardScaler()
dataset['Time'] = sc.fit_transform(dataset['Time'].values.reshape(-1, 1))
dataset['Amount'] = sc.fit_transform(dataset['Amount'].values.reshape(-
1, 1))
#'''The last column in the dataset is our target variable.'''
raw_data = dataset.values
# The last element contains if the transaction is normal which is represente
d by a 0 and if fraud then 1
labels = raw_data[:, -1]
# The other data points are the electrocadriogram data
data = raw_data[:, 0:-1]
train_data, test_data, train_labels, test_labels = train_test_split(
 data, labels, test_size=0.2, random_state=2021
#'''Normalize the data to have a value between 0 and 1'''
min_val = tf.reduce_min(train_data)
max_val = tf.reduce_max(train_data)
train_data = (train_data - min_val) / (max_val - min_val)
test_data = (test_data - min_val) / (max_val - min_val)
train_data = tf.cast(train_data, tf.float32)
test_data = tf.cast(test_data, tf.float32)
#Use only normal transactions to train the Autoencoder.
Normal data has a value of 0 in the target variable. Using the target variab
le to create a normal and fraud dataset.'''
train_labels = train_labels.astype(bool)
test_labels = test_labels.astype(bool)
Page 5 of 13
normal_train_data = train_data[~train_labels]
normal_test_data = test_data[~test_labels]
fraud_train_data = train_data[train_labels]
fraud_test_data = test_data[test_labels]
print(" No. of records in Fraud Train Data=",len(fraud_train_data))
print(" No. of records in Normal Train data=",len(normal_train_data))
print(" No. of records in Fraud Test Data=",len(fraud_test_data))
print(" No. of records in Normal Test data=",len(normal_test_data))
nb_epoch = 50
batch_size = 64
input_dim = normal_train_data.shape[1] #num of columns, 30
encoding_dim = 14
hidden_dim_1 = int(encoding_dim / 2) #
hidden_dim_2=4 
learning_rate = 1e-7
#input Layer
input_layer = tf.keras.layers.Input(shape=(input_dim, ))
#Encoder
encoder = tf.keras.layers.Dense(encoding_dim, activation="tanh", 
 
 activity_regularizer=tf.keras.regularizers.l2(learni
ng_rate))(input_layer)
encoder=tf.keras.layers.Dropout(0.2)(encoder)
encoder = tf.keras.layers.Dense(hidden_dim_1, activation='relu')(encoder)
encoder = tf.keras.layers.Dense(hidden_dim_2, activation=tf.nn.leaky_relu)(e
ncoder)
# Decoder
decoder = tf.keras.layers.Dense(hidden_dim_1, activation='relu')(encoder)
decoder=tf.keras.layers.Dropout(0.2)(decoder)
decoder = tf.keras.layers.Dense(encoding_dim, activation='relu')(decoder)
decoder = tf.keras.layers.Dense(input_dim, activation='tanh')(decoder)
#Autoencoder
autoencoder = tf.keras.Model(inputs=input_layer, outputs=decoder)
autoencoder.summary()
#""Define the callbacks for checkpoints and early stopping"""
cp = tf.keras.callbacks.ModelCheckpoint(filepath="autoencoder_fraud.h5",
 mode='min', monitor='val_loss', verbose=2, sa
ve_best_only=True)
# define our early stopping
early_stop = tf.keras.callbacks.EarlyStopping(
 monitor='val_loss',
 min_delta=0.0001,
 patience=10,
 verbose=1,
Page 6 of 13
 mode='min',
 restore_best_weights=True)
#Compile the Autoencoder
autoencoder.compile(metrics=['accuracy'],
 loss='mean_squared_error',
optimizer='adam')
#Train the Autoencoder
history = autoencoder.fit(normal_train_data, normal_train_data,
 epochs=nb_epoch,
batch_size=batch_size,
shuffle=True,
 validation_data=(test_data, test_data),
 verbose=1,
callbacks=[cp, early_stop]
).history
#Plot training and test loss
plt.plot(history['loss'], linewidth=2, label='Train')
plt.plot(history['val_loss'], linewidth=2, label='Test')
plt.legend(loc='upper right')
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
#plt.ylim(ymin=0.70,ymax=1)
plt.show()
#"""Detect Anomalies on test data
Anomalies are data points where the reconstruction loss is higher
To calculate the reconstruction loss on test data,
predict the test data and calculate the mean square error between the test d
ata and the reconstructed test data."""
test_x_predictions = autoencoder.predict(test_data)
mse = np.mean(np.power(test_data - test_x_predictions, 2), axis=1)
error_df = pd.DataFrame({'Reconstruction_error': mse,
 'True_class': test_labels})
#Plotting the test data points and their respective reconstruction error set
s a threshold value to visualize
#if the threshold value needs to be adjusted.
threshold_fixed = 50
groups = error_df.groupby('True_class')
fig, ax = plt.subplots()
for name, group in groups:
Page 7 of 13
 ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, lin
estyle='',
 label= "Fraud" if name == 1 else "Normal")
ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors="r", z
order=100, label='Threshold')
ax.legend()
plt.title("Reconstruction error for normal and fraud data")
plt.ylabel("Reconstruction error")
plt.xlabel("Data point index")
plt.show();
'''Detect anomalies as points where the reconstruction loss is greater than
a fixed threshold.
Here we see that a value of 52 for the threshold will be good.
Evaluating the performance of the anomaly detection'''
threshold_fixed =52
pred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_e
rror.values]
error_df['pred'] =pred_y
conf_matrix = confusion_matrix(error_df.True_class, pred_y)
plt.figure(figsize=(4, 4))
sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True,
fmt="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()
# print Accuracy, precision and recall
print(" Accuracy: ",accuracy_score(error_df['True_class'], error_df['pred'])
)
print(" Recall: ",recall_score(error_df['True_class'], error_df['pred']))
print(" Precision: ",precision_score(error_df['True_class'], error_df['pred'
]))
#'''As our dataset is highly imbalanced, we see a high accuracy but a low re
call and precision.
Things to further improve precision and recall would add more relevant featu
res,
different architecture for autoencoder, different hyperparameters, or a diff
erent algorithm.'''





*********************************** Practical No 5 ******************************
Continuous Bag of Words (CBOW)


import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
import matplotlib.pylab as pylab
import numpy as np
%matplotlib inline
#Data Prepration
import re
sentences = """We are about to study the idea of a computational process.
Computational processes are abstract beings that inhabit computers.
As they evolve, processes manipulate other abstract things called data.
The evolution of a process is directed by a pattern of rules
called a program. People create programs to direct processes. In effect,
we conjure the spirits of the computer with our spells."""
Clean Data
# remove special characters
sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)
# remove 1 letter words
sentences = re.sub(r'(?:^| )\w(?:$| )', ' ', sentences).strip()
# lower all characters
sentences = sentences.lower()
Vocabulary
words = sentences.split()
vocab = set(words)
vocab_size = len(vocab)
embed_dim = 10
context_size = 2
Implementation
word_to_ix = {word: i for i, word in enumerate(vocab)}
ix_to_word = {i: word for i, word in enumerate(vocab)}
Data bags
# data - [(context), target]
data = []
for i in range(2, len(words) - 2):
context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]
target = words[i]
data.append((context, target))
print(data[:5])
[(['we', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['a
Embeddings
embeddings = np.random.random_sample((vocab_size, embed_dim))
Linear Model
def linear(m, theta):
w = theta
return m.dot(w)
Log softmax + NLLloss = Cross Entropy
def log_softmax(x):
e_x = np.exp(x - np.max(x))
return np.log(e_x / e_x.sum())
def NLLLoss(logs, targets):
out = logs[range(len(targets)), targets]
return -out.sum()/len(out)
def log_softmax_crossentropy_with_logits(logits,target):
out = np.zeros_like(logits)
out[np.arange(len(logits)),target] = 1
softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)
return (- out + softmax) / logits.shape[0]
Forward function
def forward(context_idxs, theta):
m = embeddings[context_idxs].reshape(1, -1)
n = linear(m, theta)
o = log_softmax(n)
return m, n, o
Backward function
def backward(preds, theta, target_idxs):
m, n, o = preds
dlog = log_softmax_crossentropy_with_logits(n, target_idxs)
dw = m.T.dot(dlog)
return dw
Optimize function
def optimize(theta, grad, lr=0.03):
theta -= grad * lr
return theta
Training
#Genrate training data
theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))
epoch_losses = {}
for epoch in range(80):
losses = []
for context, target in data:
context_idxs = np.array([word_to_ix[w] for w in context])
preds = forward(context_idxs, theta)
target_idxs = np.array([word_to_ix[target]])
loss = NLLLoss(preds[-1], target_idxs)
losses.append(loss)
grad = backward(preds, theta, target_idxs)
theta = optimize(theta, grad, lr=0.03)
epoch_losses[epoch] = losses
Analyze
Plot loss/epoch
Text(0, 0.5, 'Losses')
ix = np.arange(0,80)
fig = plt.figure()
fig.suptitle('Epoch/Losses', fontsize=20)
plt.plot(ix,[epoch_losses[i][0] for i in ix])
plt.xlabel('Epochs', fontsize=12)
plt.ylabel('Losses', fontsize=12)
Predict function
def predict(words):
context_idxs = np.array([word_to_ix[w] for w in words])
preds = forward(context_idxs, theta)
word = ix_to_word[np.argmax(preds[-1])]
return word
# (['we', 'are', 'to', 'study'], 'about')
predict(['we', 'are', 'to', 'study'])
check 0s completed at 2:44 PM
Colab paid products - Cancel contracts here
'
about
'
Accuracy
def accuracy():
wrong = 0
for context, target in data:
if(predict(context) != target):
wrong += 1
return (1 - (wrong / len(data)))
accuracy()
1.0
'
other
'
predict(['processes', 'manipulate', 'things', 'study'])











import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

BATCH_SIZE = 32
IMG_SIZE = (160, 160)

train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,shuffle=True,batch_size=BATCH_SIZE,image_size=IMG_SIZE)
validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,
                                                                 shuffle=True,
                                                                 batch_size=BATCH_SIZE,
                                                                 image_size=IMG_SIZE)
class_names = train_dataset.class_names

plt.figure(figsize=(10, 10))
for images, labels in train_dataset.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")
 val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))
print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)

data_augmentation = tf.keras.Sequential([
  tf.keras.layers.RandomFlip('horizontal'),
  tf.keras.layers.RandomRotation(0.2),
])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
# Create the base model from the pre-trained model
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
image_batch, label_batch = next(iter(train_dataset))
feature_batch = base_model(image_batch)
print(feature_batch.shape)

(32, 5, 5, 1280)
base_model.trainable = False














************************************** Practical No 6 ****************************************
To implement Object detection using Transfer Learning of CNN architectures


import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

BATCH_SIZE = 32
IMG_SIZE = (160, 160)

train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,shuffle=True,batch_size=BATCH_SIZE,image_size=IMG_SIZE)
validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,
                                                                 shuffle=True,
                                                                 batch_size=BATCH_SIZE,
                                                                 image_size=IMG_SIZE)
class_names = train_dataset.class_names

plt.figure(figsize=(10, 10))
for images, labels in train_dataset.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")
 val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))
print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)

data_augmentation = tf.keras.Sequential([
  tf.keras.layers.RandomFlip('horizontal'),
  tf.keras.layers.RandomRotation(0.2),
])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
# Create the base model from the pre-trained model
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
image_batch, label_batch = next(iter(train_dataset))
feature_batch = base_model(image_batch)
print(feature_batch.shape)

(32, 5, 5, 1280)
base_model.trainable = False




Model: "mobilenetv2_1.00_160"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 160, 160, 3  0           []                               
                                )]                                                                
                                                                                                  
 Conv1 (Conv2D)                 (None, 80, 80, 32)   864         ['input_1[0][0]']                
                                                                                                  
 bn_Conv1 (BatchNormalization)  (None, 80, 80, 32)   128         ['Conv1[0][0]']                  
                                                                                                  
 Conv1_relu (ReLU)              (None, 80, 80, 32)   0           ['bn_Conv1[0][0]']               
                                                                                                  
 expanded_conv_depthwise (Depth  (None, 80, 80, 32)  288         ['Conv1_relu[0][0]']             
 wiseConv2D)                                                                                      
                                                                                                  
 expanded_conv_depthwise_BN (Ba  (None, 80, 80, 32)  128         ['expanded_conv_depthwise[0][0]']
 tchNormalization)                                                                                
                                                                                                  
 expanded_conv_depthwise_relu (  (None, 80, 80, 32)  0           ['expanded_conv_depthwise_BN[0][0
 ReLU)                                                           ]']                              
                                                                                                  
 expanded_conv_project (Conv2D)  (None, 80, 80, 16)  512         ['expanded_conv_depthwise_relu[0]
                                                                 [0]']                            
                                                                                                  
...
Total params: 2,257,984
Trainable params: 0
Non-trainable params: 2,257,984
inputs = tf.keras.Input(shape=(160, 160, 3))
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(x, training=False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

 
# Retrieve a batch of images from the test set
image_batch, label_batch = test_dataset.as_numpy_iterator().next()
predictions = model.predict_on_batch(image_batch).flatten()

# Apply a sigmoid since our model returns logits
predictions = tf.nn.sigmoid(predictions)
predictions = tf.where(predictions < 0.5, 0, 1)

print('Predictions:\n', predictions.numpy())
print('Labels:\n', label_batch)

plt.figure(figsize=(10, 10))
for i in range(9):
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(image_batch[i].astype("uint8"))
  plt.title(class_names[predictions[i]])
  plt.axis("off")

 

